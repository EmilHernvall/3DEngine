Writing a 3D engine using the rasterization approach
Emil Hernvall

2011-09-18, day 1

    My main resource so far has been http://en.wikipedia.org/wiki/Perspective_transform
    
    I decided to start out with a engine without any support for surfaces, by simply
    drawing a mesh. This was accomplished rather easily using the math from the article.
    
    The math seems intuitive enough, and I think I understand most of it. Right now I:
    
    1. Transform the coordinate of the point being projected to a system with the camera
       at the origin.
    2. Apply the rotation matrices. I currently on perform a rotation around the x-axis
       and z-axis. A transformation around the y-axis would be analogues to tilting
       ones head. This might be interesting later as part of the game dynamics, but
       it's not usually something that players are allowed to control. Maybe it could
       be used to allow people to lean out from behind walls? Perhaps I should develop
       this into a multi player fps? :)
    3. Perform the perspective transform. This turned out to be more straight forward
       than I imagined. It's simply a matter of scaling the points based on the distance
       from the screen.
    4. This confused me for a bit. The resulting points will be centered around the origin
       so they need to be shifted by half of the screen size.
    
    The most confusing thing so far was that it took me a while to realize that the camera
    movements had to depend on the angle of the camera. I solved this by expressing the
    movement as simple vectors and applying the same rotation matrices that I use for
    transforming the coordinates of vertices when performing the projection.
    
    The approach I currently use for keyboard input isn't working very well. I find it
    especially annoying that pressing multiple keys simultaneously doesn't work.
    Some kind of async polling should probably do the trick. Combining different movements
    should be rather easy since it's just a question of vector addition.
    
    I'm a bit confused about the focal length of the virtual camera. The current value
    seems to work well enough. Lower values seemed to cause an ugly wide angle look.
    Interestingly enough my current value is equal to the size of the projection area.
    It seems likely that there is some significance to that.

    Tomorrows project will involve using a z-buffer to project the different surfaces
    and adding shading. I'm probably going to use the phong reflection model, and 
    try out several different approaches to shading. The three main ones seem to be
    Flat shading, Gouraud shading and Phong shading. I'll also look into texture
    mapping.

2011-09-19, day 2

    Started out by taking care of a few things that doesn't require any major research.
    I moved to rendering at a fixed fps and checking for movement right before redrawing
    the scene. This seems to work well enough, and navigating is now a lot more pleasant.
    The problem with vertices behind the camera still being drawn was also solved.
    
    I still plan to move on to using a z-buffer, but for now I've managed to give a solid
    feel to the surfaces by simply sorting by the y-centroid. This causes some annoying
    glitches, but since I would have to start drawing polygons by hand if I want to use
    a z-buffer, this seems like a good solution for now.
    
    The phong reflectance model is rather complicated, so I decided that it would be
    better to start with something simpler. Lambertian reflectance seemed to be the
    simplest model, since it only involved taking the dot product of the surface normal
    and the light source and multiplying it by an intensity value. This seems to give
    reasonable results. The problem is that this obviously doesn't take visibility
    into account, so hidden surfaces are incorrectly shaded. I think I'll need
    some more complex objects to be able to test this correctly. Maybe I can interpolate
    a sphere?
    
2011-09-20, day 3

    Finally got around to implementing triangle drawing and moving to zbuffering. There
    are a few glitches that I'm unsure about, but in principle things seem to work. I think
    it might be related to precision issues due to casting. The triangle drawing routines 
    does a whole lot of interpolation which seems to be rather resource intensive, so I'll 
    try to optimize that later.
    
    I've been annoyed for a few days about the engine being tied into Swing and AWT. I finally
    got around to moving the platform specific code into a separate surface class. This should
    in principle make it easy to move to other surfaces. Possibly an HTML 5 canvas? I'm
    guessing that the entire project could be compiled in GWT, which would be kind of cool.
    
2011-09-21, day 4

    Decided it was time to move on to some for complex surfaces, and opted for spheres. Used
    spherical coordinates to construct the sphere by doing a fixed number of steps and creating
    polygons along the way.
    
    One of the things that's been nagging me was the fact that surfaces on the side of an object
    opposite the light source was still lit. I realized yesterday that one solution might be
    to use some kind of oriented surfaces that are only rendered if viewed from a certain
    direction, but for some reason I didn't manage to get that working. Turns out that all I
    had to do was to take the dot product of the polygon normal and the vector from a point
    on the polygon to the camera, and check if the result is positive. This solved a lot
    of glitches, and should probably have been done a lot earlier.
    
    I also decided to try gouraud shading rather than flat shading. This scheme is simple
    enough conceptually, and if it wasn't for any number of confusing glitches it would
    have been easy to implement. In principle, you just interpolate the normal vector
    of each polygon at a certain vertex and interpolate across the polygon. This looks
    _a lot_ better than flat shading, even though I'm still only using lambertian reflectance.
    Switching to a different reflection model doesn't seem very hard, though, so I'll
    probably give phong reflectance a shot when I have the time.
    
    Another thing I've realized today is that I really need to implement some kind of
    clipping strategy. Sutherland–Hodgman seems like a good choice, and doesn't seem
    too complicated to implement.
    
2011-09-21, day 5

    This is a dump of resources I've been using.
    
    Drawing triangles. This is pretty much the only code I've copied verbatim, although I've
    had to modify to interpolate color intensity and the z-coordinate. It's pretty close to
    what I would have done if I had implemented it myself, and the code is easy enough to
    understand:
    
     * http://gabrielongraphics.blogspot.com/2005/09/drawing-triangles.html
     * http://gabrielongraphics.blogspot.com/2005/09/drawing-line-segments.html 
    
    Shading and lighting:
    
     * http://en.wikipedia.org/wiki/Lambertian_reflectance
     * http://en.wikipedia.org/wiki/Gouraud_shading
     
     * http://en.wikipedia.org/wiki/Phong_reflection_model
     * http://en.wikipedia.org/wiki/Phong_shading
     
    Clipping:
    
     * http://en.wikipedia.org/wiki/Sutherland-Hodgeman
     
    I got some key insight from here, in regards to hiding polygons facing away from the camera:
    
     * http://www.flipcode.com/archives/Building_a_3D_Portal_Engine-Issue_06_Hidden_Surface_Removal.shtml
     
    It was high time to do some optimization, so I profiled the code using JVisualVM and realized that almost
    all time is spent drawing the individual pixels. I handled this by moving away from the crappy solution
    of using fillRect() to set the pixels, and replaced it with a image so that I can write directly to the
    raster. This was a major speed up.
    
    Since none of the lighting calculations depend on the camera position right now, I was also able to
    move those calculations into a preprocessing step, which also caused a major speed up.
    
    I was a bit surprised to learn that very little time is spent in the linear interpolation routine,
    despite the rather extensive use I make of it. Right now the code runs rather fast, which feels rather
    satisfying. The next step will probably be to implement some kind of clipping against the view frustum.
    